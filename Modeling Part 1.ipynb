{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8GuuGzrkJRoa"
   },
   "source": [
    "# SET UP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "id": "d7YytX2BJRog",
    "outputId": "8594c98e-b42d-4b1a-96f8-ba8cbd5765b9"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m W&B installed but not logged in.  Run `wandb login` or set the WANDB_API_KEY env variable.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        if (typeof require !== 'undefined') {\n",
       "        require.undef(\"plotly\");\n",
       "        requirejs.config({\n",
       "            paths: {\n",
       "                'plotly': ['https://cdn.plot.ly/plotly-latest.min']\n",
       "            }\n",
       "        });\n",
       "        require(['plotly'], function(Plotly) {\n",
       "            window._Plotly = Plotly;\n",
       "        });\n",
       "        }\n",
       "        </script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "        <script type=\"text/javascript\">\n",
       "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
       "        if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
       "        if (typeof require !== 'undefined') {\n",
       "        require.undef(\"plotly\");\n",
       "        requirejs.config({\n",
       "            paths: {\n",
       "                'plotly': ['https://cdn.plot.ly/plotly-latest.min']\n",
       "            }\n",
       "        });\n",
       "        require(['plotly'], function(Plotly) {\n",
       "            window._Plotly = Plotly;\n",
       "        });\n",
       "        }\n",
       "        </script>\n",
       "        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('fivethirtyeight')\n",
    "from pylab import rcParams\n",
    "rcParams['figure.figsize'] = 20, 8\n",
    "\n",
    "import math\n",
    "from statsmodels.tsa.stattools import acf,pacf\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import math\n",
    "\n",
    "from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\n",
    "import plotly as py\n",
    "import plotly.io as pio\n",
    "import plotly.graph_objs as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "\n",
    "from models import * \n",
    "from utils import * \n",
    "\n",
    "# Show charts when running kernel\n",
    "init_notebook_mode(connected=True)\n",
    "\n",
    "# Change default background color for all visualizations\n",
    "layout=go.Layout(paper_bgcolor='rgba(0,0,0,0)', plot_bgcolor='rgba(250,250,250,0.8)')\n",
    "fig = go.Figure(layout=layout)\n",
    "templated_fig = pio.to_templated(fig)\n",
    "pio.templates['my_template'] = templated_fig.layout.template\n",
    "pio.templates.default = 'my_template'\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3KDL4alMJRoq"
   },
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "2wWdiysddBXH"
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('preprocessed_data.csv')\n",
    "hydro = data[data['ID'] == '05RE003']\n",
    "hydro['Water Level_previous'] = hydro[\"Value\"].shift(1) \n",
    "hydro['WL_pct'] = 100*(hydro[\"Value\"]  - hydro['Water Level_previous'] ) / hydro['Water Level_previous']\n",
    "hydro.dropna(inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IH6ZkfoKJRoq"
   },
   "source": [
    "## Statistical Features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "kclt08RZJRoq"
   },
   "outputs": [],
   "source": [
    "hydro['WL_mean_12'] = hydro['WL_pct'].rolling(12).mean()\n",
    "hydro['WL_mean_18'] = hydro['WL_pct'].rolling(18).mean()\n",
    "hydro['WL_mean_24'] = hydro['WL_pct'].rolling(24).mean()\n",
    "\n",
    "hydro['WL_std_12']  = hydro['WL_pct'].rolling(12).std()\n",
    "hydro['WL_std_24']  = hydro['WL_pct'].rolling(24).std()\n",
    "\n",
    "hydro['WL_lag_12']  = hydro['WL_pct'].shift(12)\n",
    "hydro['WL_lag_6']  = hydro['WL_pct'].shift(6)\n",
    "\n",
    "hydro.dropna(inplace = True )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "ZlEH3q85JRor"
   },
   "outputs": [],
   "source": [
    "num_feat = ['WL_pct','WL_mean_12','WL_mean_18','WL_mean_18','WL_mean_24','WL_std_12','WL_std_24','WL_lag_12','WL_lag_6']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9orGYXdnJRor"
   },
   "source": [
    "## Time Based Features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "un5Rh00WJRor"
   },
   "outputs": [],
   "source": [
    "hydro['Date'] = pd.to_datetime(hydro['Date'])\n",
    "hydro['hour'] = hydro['Date'].dt.hour\n",
    "hydro['dayOfweek'] =  hydro['Date'].dt.dayofweek\n",
    "hydro['month'] =  hydro['Date'].dt.month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "xOr60DA9JRor"
   },
   "outputs": [],
   "source": [
    "cat_feat = ['hour','dayOfweek','month']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "FEjnhrYLJRos"
   },
   "outputs": [],
   "source": [
    "embbedded_size  = [(24,12),(7,4),(12,6)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D1Y4V1MGJRos"
   },
   "source": [
    "# Modling Part "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "kRe4qMI1JRos"
   },
   "outputs": [],
   "source": [
    "seq_len = len(hydro)\n",
    "train_data = hydro.iloc[:int(0.8*seq_len)] \n",
    "val_data = hydro.iloc[int(0.8*seq_len):] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "90JjQTQmJRos"
   },
   "source": [
    "## Stacked LSTM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "ACGykTEtJRot",
    "outputId": "4ceaf237-c42c-4815-d69f-968826fb0c6f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------- Epoch 0 ---------\n",
      " train_loss  = 0.004113546538164783\n",
      " val_loss  = 0.00015200092688650733\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-523d5aebc190>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0mverbose\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m     \u001b[0mis_forcaster\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m )\n",
      "\u001b[0;32m/kaggle/working/Water-Level-Forecasting/utils/engine.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(model, train_dataset, valid_dataset, lr, EPOCHS, TRAIN_BATCH_SIZE, VALID_BATCH_SIZE, device, path, verbose, is_forcaster, is_added_auto_encoder)\u001b[0m\n\u001b[1;32m    208\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_forcaster\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m             tr_loss = train_fn_forcaster(\n\u001b[0;32m--> 210\u001b[0;31m                 \u001b[0mtrain_data_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_added_auto_encoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    211\u001b[0m             )\n\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/kaggle/working/Water-Level-Forecasting/utils/engine.py\u001b[0m in \u001b[0;36mtrain_fn_forcaster\u001b[0;34m(data_loader, model, optimizer, device, verbose, is_added_auto_encoder)\u001b[0m\n\u001b[1;32m    100\u001b[0m         \u001b[0mcounter\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m             \u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/transformers/optimization.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    294\u001b[0m                 \u001b[0;31m# In-place operations to update the averages at the same time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m                 \u001b[0mexp_avg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1.0\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 296\u001b[0;31m                 \u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddcmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1.0\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    297\u001b[0m                 \u001b[0mdenom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"eps\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Hyperparameters \n",
    "train_batch_size = 32\n",
    "valid_batch_size = 16\n",
    "epochs = 100\n",
    "input_size = len(num_feat)\n",
    "hidden_size = 32\n",
    "lr = 1e-4\n",
    "device = torch.device('cuda')\n",
    "window_size = 24  \n",
    "path = 'LSTM_Staked.pth'\n",
    "verbose = False\n",
    "\n",
    "train_dataset = hydro_dataset(train_data , window_size , num_feat , cat_feat )\n",
    "valid_dataset = hydro_dataset(val_data , window_size , num_feat , cat_feat )\n",
    "\n",
    "stacked_lstm = StackedLSTMs(input_size,hidden_size)\n",
    "stacked_lstm = stacked_lstm.to(device)\n",
    "val_loss, train_loss = run(\n",
    "    stacked_lstm,\n",
    "    train_dataset,\n",
    "    valid_dataset,\n",
    "    lr,\n",
    "    epochs,\n",
    "    train_batch_size,\n",
    "    valid_batch_size,\n",
    "    device,\n",
    "    path,\n",
    "    verbose , \n",
    "    is_forcaster = True \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "spL5zXeB3zpf"
   },
   "source": [
    "# Auto Encoder + LSTM Stacked "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f7BVP3Bm7YI6",
    "outputId": "4230cbd0-4909-4b42-8489-849e0bbfaa41"
   },
   "outputs": [],
   "source": [
    "# Hyperparameters \n",
    "train_batch_size = 128\n",
    "valid_batch_size = 64\n",
    "epochs = 200\n",
    "input_size = len(num_feat)\n",
    "hidden_size = 32\n",
    "lr = 5e-4\n",
    "device = torch.device('cuda')\n",
    "window_size = 24  \n",
    "path = 'LSTM_Staked.pth'\n",
    "verbose = False\n",
    "emb_size = 8\n",
    "\n",
    "train_dataset = hydro_dataset(train_data , window_size , num_feat , cat_feat )\n",
    "valid_dataset = hydro_dataset(val_data , window_size , num_feat , cat_feat )\n",
    "\n",
    "autoencoder_lstm = LSTMAutoEncoder(input_size = input_size ,hidden_size = hidden_size,emb_size= emb_size )\n",
    "autoencoder_lstm = autoencoder_lstm.to(device)\n",
    "\n",
    "val_loss, train_loss = run(\n",
    "    autoencoder_lstm,\n",
    "    train_dataset,\n",
    "    valid_dataset,\n",
    "    lr,\n",
    "    epochs,\n",
    "    train_batch_size,\n",
    "    valid_batch_size,\n",
    "    device,\n",
    "    path,\n",
    "    verbose , \n",
    "    is_forcaster = True , \n",
    "    is_added_auto_encoder = True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_HICvxACJkKq"
   },
   "source": [
    "## Evaluation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2kPlMnaJuwER"
   },
   "outputs": [],
   "source": [
    "def mean_absolute_percentage_error(y_true, y_pred): \n",
    "    y_true, y_pred = np.array(y_true), np.array(y_pred)\n",
    "    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "68PV7Q0WJkB8",
    "outputId": "abcf97c2-fbcd-46b7-f3f8-4192f451b3b0"
   },
   "outputs": [],
   "source": [
    "val_data = val_data.iloc[window_size:]\n",
    "\n",
    "pred = predict( stacked_lstm , valid_dataset)\n",
    "val_data['WL_pct_hat'] = pred / 100\n",
    "val_data['WL_hat_stacked_lstm'] = val_data['Water Level_previous'] * (1+val_data['WL_pct_hat'] )\n",
    "\n",
    "pred = predict( autoencoder_lstm , valid_dataset ,is_added_auto_encoder = True )\n",
    "val_data['WL_pct_hat'] = pred / 100\n",
    "val_data['WL_hat_autoencoder_lstm'] = val_data['Water Level_previous'] * (1+val_data['WL_pct_hat'] )\n",
    "\n",
    "fig = make_subplots()\n",
    "fig.add_trace(go.Scatter( y=val_data['Value'].values, name='Ground Truth values'))\n",
    "fig.add_trace(go.Scatter( y=val_data['WL_hat_stacked_lstm'].values ,name='stacked_lstm predictions'))\n",
    "fig.add_trace(go.Scatter( y=val_data['WL_hat_autoencoder_lstm'].values ,name='LSTM + AutoEncoder  predictions'))\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w_Dii7Nm_grT",
    "outputId": "f334c9da-766f-4965-9928-a66354a79415"
   },
   "outputs": [],
   "source": [
    "plt.plot( val_data['Value'].values, label='Ground Truth values')\n",
    "plt.plot( val_data['WL_hat_stacked_lstm'].values ,label='stacked_lstm predictions')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q5P7iilHmt3B",
    "outputId": "89b476b3-8f04-4f6a-e9b9-f4b16c605915"
   },
   "outputs": [],
   "source": [
    "plt.plot( val_data['Value'].values, label='Ground Truth values')\n",
    "plt.plot( val_data['WL_hat_autoencoder_lstm'].values ,label=' LSTM + AutoEncoder predictions ')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g2uoEbe3_-qi",
    "outputId": "0a7e417f-b0cb-46d2-9e50-36c60402ce07"
   },
   "outputs": [],
   "source": [
    "print('LSTM Stacked Model Results')\n",
    "print('MSE = ' , mean_squared_error(val_data['Value'], val_data['WL_hat_stacked_lstm']))\n",
    "print('RMSE = ', math.sqrt(mean_squared_error(val_data['Value'], val_data['WL_hat_stacked_lstm'])))\n",
    "print('mean_absolute_error = ', mean_absolute_error(val_data['Value'], val_data['WL_hat_stacked_lstm']))\n",
    "print('mean_absolute_percentage_error = ', mean_absolute_percentage_error(val_data['Value'], val_data['WL_hat_stacked_lstm']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GJnSLMOQtCe7",
    "outputId": "c9c41513-0738-4d95-a3f7-876e589ccef5"
   },
   "outputs": [],
   "source": [
    "print('LSTM + AutoEncoder Model Results')\n",
    "print('MSE = ' , mean_squared_error(val_data['Value'], val_data['WL_hat_autoencoder_lstm']))\n",
    "print('RMSE = ', math.sqrt(mean_squared_error(val_data['Value'], val_data['WL_hat_autoencoder_lstm'])))\n",
    "print('mean_absolute_error = ', mean_absolute_error(val_data['Value'], val_data['WL_hat_autoencoder_lstm']))\n",
    "print('mean_absolute_percentage_error = ', mean_absolute_percentage_error(val_data['Value'], val_data['WL_hat_autoencoder_lstm']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
